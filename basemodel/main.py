from model_loader import model, tokenizer
from generate import generate_output
from evaluate_relevance import evaluate_relevance
import torch
import random

# ã‚·ãƒ¼ãƒ‰å›ºå®š
random.seed(0)

# CUDA or CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

question = "ç£¯æ‘æ‹“å“‰ãƒ¦ãƒ‹ãƒƒãƒˆãƒªãƒ¼ãƒ€ãƒ¼ã®è«–æ–‡ã€Œè„³çŸ¥èƒ½ã®ä¸‰å¤§ç†è«–ã‚’çµ±åˆã™ã‚‹ä¸‰é‡ç­‰ä¾¡æ€§ã€ã®è¦ç´„ã‚’ã—ã¦ãã ã•ã„"

response = generate_output(question, model, tokenizer)
print("ğŸ—£ï¸ å›ç­”:\n", response)

evaluation = evaluate_relevance(question, response, model, tokenizer)
print("ğŸ“Š å›ç­”ã®é–¢é€£æ€§è©•ä¾¡:\n", evaluation)

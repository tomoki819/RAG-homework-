Title: Triple equivalence for the emergence of biological intelligence



## Abstract
Intelligent algorithms developed evolutionarily within neural systems are considered in this work. Mathematical analyses unveil a triple equivalence between canonical neural networks, variational Bayesian inference under a class of partially observable Markov decision processes, and differentiable Turing machines, by showing that they minimise the shared Helmholtz energy. Consequently, canonical neural networks can biologically plausibly perform variational Bayesian inferences of external Turing machines. Applying Helmholtz energy minimisation at the species level facilitates deriving active Bayesian model selection inherent in natural selection, resulting in the emergence of adaptive algorithms. Canonical neural networks with two mental actions can form a universal machine by separately memorising transition mappings of multiple external Turing machines. These propositions are corroborated by numerical simulations of algorithm implementation and neural network evolution. These notions offer a universal characterisation of biological intelligence emerging from evolution in terms of Bayesian model selection and belief updating.

## Introduction
Characterising the intelligence of biological organisms is challenging yet crucial. Although a conclusive definition of intelligence remains to be established, various intelligent functions are computable and can be implemented as algorithms1. This might be obvious considering that the dynamics of the fundamental units of the brain—i.e., neurons2,3,4 and synapses5,6,7—can be expressed as algorithms. Moreover, biological intelligence has been shaped through evolution8, driven by the preferential survival of organisms who successfully reproduce and contribute more offspring (i.e., genes) to subsequent generations. These features comprise requisites for biologically plausible algorithms. However, a precise characterisation of the biological intelligence that stems from evolution is yet to be established. This work addresses the aforementioned issue by characterising algorithms formed through natural selection in terms of variational Bayesian inference of external milieu states.The brain can be characterised in terms of dynamical systems of neural circuits9,10,11, agents that perform statistical or Bayesian inferences12,13, and machines that perform computations. Thus, constructing a theory of brain intelligence requires the integration of these perspectives. The first two are closely related to optimisations and widely expressed as the minimisation of cost or energy functions, wherein a gradient descent on cost functions furnishes biologically plausible algorithms14,15,16. The free-energy principle has been proposed to account for the perception, learning, and actions of biological organisms in terms of variational Bayesian inference and free energy minimisation17,18. Although its physiological bases have been debated, recent works have shown that any neural network that minimises its cost function can be cast as performing variational Bayesian inference19,20,21. This is attributed to the natural equivalence between the Helmholtz energy implicit in neural networks and variational free energy under a class of generative models21.Previous works have established that the dynamics of canonical neural networks are formally homologous to Bayesian belief updates under a specific yet sufficiently generic class of partially observable Markov decision processes (POMDPs). These networks have been shown to perform causal inference19, rule learning22, and planning20. Moreover, the consistency between such model dynamics and experimental observations has been demonstrated using in vitro neural networks23,24,25, wherein canonical neural networks can quantitatively predict the self-organisation of in vitro neural networks when assimilating sensory data, validating their biological plausibility25. These works licence the adoption of the free-energy principle as a universal characterisation of neural and neuronal networks.Having said this, previous computational models in neuroscience have addressed limited algorithms, which rely on specific (generative model) architectures assigned a priori and thus have limited flexibility. In contrast, any algorithm can be implemented in the form of a Turing machine1, which is a simple mathematical model representing an automatic machine that can perform any computation comprising combinations of symbolic operations. Previous works have developed artificial neural networks combined with external memory26,27 and spiking neural networks with a specific architecture28,29,30 to implement Turing machines. However, neuronal substrates of external memory—available for reading and writing—remain unelucidated. Moreover, how biological agents can evolutionarily acquire (i.e., self-organise) these architectures remains nontrivial. To characterise biological intelligence, one must commit to a class of generic algorithms and elucidate the types of algorithms shaped by evolution.To address these issues, the present work characterises biologically plausible algorithms arising naturally from evolution. Mathematical analyses reveal neural implementations of Turing machines through neural activity and synaptic plasticity, and show that these networks perform variational Bayesian inference of the states of external Turing machines. Additionally, the emergent mechanisms are explained through evolutionary perspectives. Bayes-optimal inference and decision making under a suitable generative model emerge via Bayesian model selection, inherent in natural selection. The paper is concluded by discussing possible neuronal mechanisms that realise the self-organising implementation of universal machines.

## Results
Equivalence between neural networks and Bayesian inferenceFirst, the equivalence between canonical neural networks and variational Bayesian inference was revisited to extend it to a framework including Turing machines in the next section. Previous works have derived canonical neural networks—which exhibit certain biological plausibility—from realistic neuron models2,3,4 through approximations20. In this work, a biological agent is defined as a canonical neural network comprising a two-layer recurrent network of rate-coding neurons (Fig. 1a, bottom). Upon receiving sensory inputs \({o}_{t}={({o}_{t1},\ldots ,{o}_{t{N}_{o}})}^{{{{\rm{T}}}}}\), the middle (xt) and output (yt) layer neurons generate neural activities \({x}_{t}={({x}_{t1},\ldots ,{x}_{t{N}_{x}})}^{{{{\rm{T}}}}}\) and \({y}_{t}={({y}_{t1},\ldots ,{y}_{t{N}_{y}})}^{{{{\rm{T}}}}}\), respectively. When xt and yt have a bounded range, their values can be rescaled within the range of 0 to 1. By defining a set of neural activities ut = {xt,yt}, their dynamics are given by the following differential equation:$${\dot{u}}_{t}\propto -{{{{\rm{sig}}}}}^{-1}\left({u}_{t}\right)+f\left({u}_{t-1},{o}_{t}\right)$$
                    (1)
                where sig−1 indicates the inverse of the sigmoid function sig(∙) = 1/(1 + e−(∙)) (known as the logit function) and f is an arbitrary vector function that maps ut−1 to ut given ot. The network’s internal states involve synaptic weight matrices ω = {W, K, V} and firing thresholds ϕ = {ϕx, ϕy} that parameterise f, where W and K denote feedforward and recurrent connections to the middle layer, while V denotes connections to the output layer. Actions \({\delta }_{t}={({\delta }_{t1},\ldots ,{\delta }_{t{N}_{\delta }})}^{{{{\rm{T}}}}}\) are then sampled from yt.Fig. 1: Schematic of triple equivalence for biological intelligence.a Three theoretical categories in neuroscience—dynamical systems, Bayesian inference, and algorithms—can be integrated under the variational principle. Bottom: Dynamical system perspective, in which the activity of neurons is modelled as differential equations. The environment comprises hidden states (st) and sensory inputs (ot), while a canonical neural network comprises middle (xt) and output (yt) layer neural activities. The dynamics of neural activity and plasticity are expressed as a gradient decent on Helmholtz energy \({{{\mathcal{A}}}}\). Top left: Brain as a Bayesian agent. The brain is considered to perceive the environment through Bayesian inference based on past experiences. This can be formulated by minimising variational free energy \({{{\mathcal{F}}}}\). The posterior belief that minimises \({{{\mathcal{F}}}}\) provides the best guess about the external milieu states. In terms of the POMDP, a set of external variables is denoted as \(\vartheta =\left\{{s}_{1:t},{\delta }_{1:t},A,B,C,D,E\right\}\). The variables in bold (e.g., st) denote the posterior beliefs about the corresponding variables in non-bold italics (e.g., st). Top right: The algorithmic perspective of the brain expressed as a Turing machine (TM). The internal TM comprises a finite automaton (xt) and memory or tape (V). Provided with automaton states \({x}_{t-1}{{{\mathcal{\in }}}}{{{\mathcal{X}}}}\) and memory readout \({y}_{t-1}\in {{{\mathcal{Y}}}}\), the Turing machine update states \({x}_{t}{{{\mathcal{\in }}}}{{{\mathcal{X}}}}\), write information in memory \({\varGamma }_{t}{{{\mathcal{\in }}}}{{{\mathcal{W}}}}\), and move header position \({\psi }_{t}^{y}{{{\mathcal{\in }}}}{{{\mathcal{M}}}}\), which is expressed as a transition mapping, \({{{\mathcal{X}}}}{{{\mathcal{\times }}}}{{{\mathcal{Y}}}}{{{\mathcal{\longmapsto }}}}{{{\mathcal{X}}}}{{{\mathcal{\times }}}}{{{\mathcal{W}}}}{{{\mathcal{\times }}}}{{{\mathcal{M}}}}\). The environment can also be expressed as a Turing machine (external TM). These three schematics emphasise the emergence of symmetry between the external and internal systems. b Equivalence between energies for canonical neural networks, variational Bayesian inference, and differentiable Turing machines, depicted by one-to-one correspondences of their components. Here, \(\bar{{x}_{t}}=\vec{1}-{x}_{t}\) denotes the sign-flipped neural activity, \({\hat{W}}_{1}={{{\rm{sig}}}}\left({W}_{1}\right)\) denotes the sigmoid function of synaptic weights, \({h}_{1}^{x}={{{\mathrm{ln}}}}\,\overline{{\hat{W}}_{1}}+{\phi }_{1}\) denotes the firing threshold, ⊙ denotes the Hadamard (i.e., elementwise) product operator, \({{{{\mathbf{\psi }}}}}_{t}^{s}\) and \({{{{\mathbf{\psi }}}}}_{t}^{\delta }\) are basis functions, and \({{{{\bf{B}}}}}^{+}={{{{\bf{B}}}}}^{{{{\rm{T}}}}}{{{\rm{diag}}}}{\left[{{{\bf{D}}}}\right]}^{-1}\) and \({{{{\bf{C}}}}}^{+}={{{{\bf{C}}}}}^{{{{\rm{T}}}}}{{{\rm{diag}}}}{\left[{{{\bf{E}}}}\right]}^{-1}\) are inverse mappings (see Methods for details). A differentiable Turing machine can be derived as gradient descent on the Helmholtz energy that is shared with canonical neural networks (see main text). Therefore, the three perspectives can be unified in terms of the variational principle—a notion referred to as triple equivalence.Full size imageThis model facilitates the identification of implicit Hamiltonian and Helmholtz energy in neural networks through reverse engineering19,20,21. For simplicity, the environment is expressed as a discrete state space, where ot and δt are vectors of binary values. A time series or path of observations is denoted as o1:t = {o1,…,ot}, and the internal states of the system—including the paths of states and parameters—are denoted as φ. Without the loss of generality, Eq. (1) can be cast as a gradient descent on the nonsteady-state Helmholtz energy21:$${{{\mathcal{A}}}}\left[\pi \left(\varphi \right),{o}_{1:t},\xi \right]={\left\langle {{{{\mathcal{H}}}}}_{\xi }\left({o}_{1:t},\varphi \right)+\frac{1}{\beta }{{{\mathrm{ln}}}} \, \pi \left(\varphi \right)\right\rangle }_{\pi \left(\varphi \right)}$$
                    (2)
                where \({{{{\mathcal{H}}}}}_{\xi }\) denotes the Hamiltonian characterised by gene ξ, 〈∙〉π(φ) is the expectation over internal state distribution π(φ), and the fixed inverse temperature β = 1 is adopted for simplicity. The minimisation of \({{{\mathcal{A}}}}\) yields the steady-state distribution π(φ). Following the treatment in previous works19,20, the explicit form of \({{{\mathcal{A}}}}\) can be reverse engineered by computing the integral of the right-hand side of Eq. (1) with respect to ut:$${{{\mathcal{A}}}}\left[\pi \left(\varphi \right),{o}_{1:t},\xi \right]={\sum }_{\tau =1}^{t}{\left(\begin{array}{c}{u}_{t}\\ \overline{{u}_{t}}\end{array}\right)}^{{{{\rm{T}}}}}\left\{{{{\mathrm{ln}}}}\,\,\left(\begin{array}{c}{u}_{t}\\ \overline{{u}_{t}}\end{array}\right)-\left(\begin{array}{c}{f}_{1}\left({u}_{t-1},{o}_{t}\right)\\ {f}_{0}\left({u}_{t-1},{o}_{t}\right)\end{array}\right)\right\}{{+}}{{{\mathcal{C}}}}$$
                    (3)
                where f1 and f0 are functions that satisfy f1–f0 ≡ f, \(\bar{{u}_{t}}=\vec{1}-{u}_{t}\) is the sign-flipped ut centred on 1/2, \(\vec{1}\) is a vector of ones, and \({{{\mathcal{C}}}}\) denotes the integration constant, which is tuned to satisfy \(\int {e}^{{{-}}{{{\mathcal{A}}}}}d{o}_{1:t}=1\) in the steady state. The transformation Eq. (1) to Eq. (3) is straightforward, as the integral of sig−1 (ut) yields \({u}_{t}^{{{{\rm{T}}}}}{{{\mathrm{ln}}}}\,\,{u}_{t}+{\bar{{u}_{t}}}^{{{{\rm{T}}}}}{{{\mathrm{ln}}}}\,\,\bar{{u}_{t}}\) and \(f\left({u}_{t-1},{o}_{t}\right)\) becomes a coefficient of the linear term of ut in the integral. The distribution π(φ) is encoded or parameterised by a set of internal variables \(\left\{{u}_{1:t},\omega ,\phi \right\}\). Owing to construction, the gradient descent on \({{{\mathcal{A}}}}\) with respect to ut, i.e., \({\dot{u}}_{t}\propto -{\partial }_{{u}_{t}}{{{\mathcal{A}}}}\), yields Eq. (1). Moreover, all synapses are updated by \(\dot{\omega }\propto -{\partial }_{\omega }{{{\mathcal{A}}}}\), which results in Hebbian plasticity with a homeostatic term. In particular, V exhibits three-factor Hebbian plasticity31,32,33 mediated by neuromodulators \({\varGamma }_{t}={({\varGamma }_{t1},\ldots ,{\varGamma }_{t{N}_{\gamma }})}^{{{{\rm{T}}}}}\) that encode risk20. Other variables are also updated by \(\dot{\phi }\propto -{\partial }_{\phi }{{{\mathcal{A}}}}\). The explicit forms of Helmholtz energy and update rules are provided in Fig. 1b and the Methods section.Crucially, according to the complete class theorem34,35,36, the admissible decision rules that minimise \({{{\mathcal{A}}}}\) can be cast as Bayesian inferences under at least one generative model with prior beliefs. Variational Bayesian inference updates prior beliefs about external milieu states Pm(ϑ) to the corresponding approximate (or exact) posterior beliefs Q(ϑ) based on o1:t (Fig. 1a, top left), wherein variational free energy, or equivalently, the negative of evidence lower bound$${{{\mathcal{F}}}}\left[Q\left(\vartheta \right),{o}_{1:t},m\right]={\left\langle -{{{\mathrm{ln}}}}\,{P}_{m}\left({o}_{1:t},\vartheta \right)+{{{\mathrm{ln}}}}\,Q\left(\vartheta \right)\right\rangle }_{Q\left(\vartheta \right)}$$
                    (4)
                is a standard cost function37. Such inferences rest upon a generative model \({P}_{m}\left({o}_{1:t},\vartheta \right)\) that expresses the agent’s hypothesis about how sensory inputs o1:t are generated from external milieu states \(\vartheta\), where subscript m explicates the underlying model structure. The gradient descent on \({{{\mathcal{F}}}}\) furnishes Bayesian belief update rules and their fixed point provides posterior belief \(Q\left(\vartheta \right)\), which approximates (or may be exactly equal to) the solution of Bayes’ theorem, \({P}_{m}\left(\vartheta |{o}_{1:t}\right)\). The same strategy can be adopted to infer the optimal actions that minimise risk (or expected free energy) in the future, which is referred to as active inference38,39,40.In essence, the complete class theorem34,35,36 and a series of recent works19,20,21 suggest the existence of an \({{{\mathcal{F}}}}\) equivalent to \({{{\mathcal{A}}}}\), that is, \({{{\mathcal{A}}}}{{\equiv }}{{{\mathcal{F}}}}\), where \(\pi \left(\varphi \right)\) encodes \(Q\left(\vartheta \right)\) and the Hamiltonian corresponds to a genetically encoded generative model, \({{{{\mathcal{H}}}}}_{\xi }\left({o}_{1:t},\varphi \right) \equiv -{{{\mathrm{ln}}}} \,{P}_{m}\left({o}_{1:t},\vartheta \right)\). Specifically, previous works have constructively shown that the dynamics of canonical neural networks formally correspond to Bayesian belief updates under a class of factorial POMDPs19,20. This is highlighted in Fig. 1b, which depicts one-to-one correspondences between components of \({{{\mathcal{A}}}}\) and \({{{\mathcal{F}}}}\). Here, \({{{\mathcal{F}}}}\) provides an upper bound of the surprise (improbability) of sensory inputs, \({{{\mathcal{F}}}} \ge - {{{\mathrm{ln}}}}\,{P}_{m}\left({o}_{1:t}\right)\), where \({P}_{m}\left({o}_{1:t}\right)=\int {P}_{m}\left({o}_{1:t},\vartheta \right)d\vartheta\) denotes the marginal likelihood. Thus, \({{{\mathcal{A}}}}\) upper bounds the surprise as well, as \({{{\mathcal{A}}}} \ge -{{{\mathrm{ln}}}}\,{P}_{m}({o}_{1:t})\), where the equality approximately holds in the steady state. This notion indicates that variational Bayesian inference and surprise minimisation are inherent properties of canonical neural networks.Triple equivalence between neural networks, Bayesian inference, and Turing machinesHere, a class of Turing machines is shown to be equivalent to canonical neural networks and variational Bayesian inference under a class of POMDPs. A Turing machine, denoted as TM(∙), is a foundational model in computational science. It comprises a finite-state machine or automaton (xt) for computations and a sufficiently long tape or large memory (V) to retain information1, enabling the expression of arbitrary algorithms (Fig. 1a, top right). For each timestep t > 0, the Turing machine reads memory value yt−1 and observation ot, updates the automaton states xt, writes a new value to the memory depending on Γt, and moves header position \({\psi }_{t}^{y}\), defined formally as$$\left\{{x}_{t},{\varGamma }_{t},{\psi }_{t}^{y}\right\}={{{\rm{TM}}}}\left({x}_{t-1},{y}_{t-1},{o}_{t}\right)$$
                    (5)
                Because Turing machines are a family of discrete state space models, Eq. (5) can be expressed in the form of POMDPs (with deterministic transition mappings), where Γt and \({\psi }_{t}^{y}\) are viewed as deterministic functions (see Methods for details). Moreover, the dynamics of ut = {xt, yt} that converge to Eq. (5) can be formulated as Eq. (1), which can be read as a differentiable Turing machine26. Such Turing machines and the corresponding Helmholtz energy \({{{\mathcal{A}}}}\) are provided by substituting \({{{\rm{sig}}}}\left(f\left({u}_{t-1},{o}_{t}\right)\right)\equiv {{{\rm{TM}}}}\left({u}_{t-1},{o}_{t}\right)\) into Eqs. (1) and (3), respectively, where the fixed point (\({\partial }_{{u}_{t}}{{{\mathcal{A}}}}=0\)) yields Eq. (5). Hence, arbitrary algorithms can be derived as the gradient descent on the implicit \({{{\mathcal{A}}}}\).Crucially, canonical neural networks and differentiable Turing machines share an identical Helmholtz energy as shown in Fig. 1b and in the Methods section. This allows differentiable Turing machines to be implemented within canonical neural networks. In this expression, the middle (xt) and output (yt) layer neurons are cast as the finite-state machine and memory readout, respectively, where yt can be viewed as a mental action that changes the network’s internal states41. The synaptic weights in the middle (K) and output (V) layers encode the transition mapping and memory, respectively, where the number of columns in V is supposed to be sufficiently large. Memory writing occurs through quick synaptic plasticity in V modulated by neuromodulator Γt, which is derived from the Helmholtz energy minimisation. This plasticity rule retains the current memory value (or strategy) yti for subsequent use when the risk is low (Γti = 0). Conversely, it forgets the current value and writes the opposite value \(\bar{{y}_{{ti}}}\) at the current memory location when the risk is high (Γti = 1). These rules can be expressed as the product of activities of pre (\(\psi_{t-1}^y\)) and post (yt) synaptic neurons and neuromodulators (\(\vec{1}-2{\varGamma }_{t}\)) in the form of a three-factor Hebbian rule31,32,33 (refer to the Discussion section for further discussion). With this architecture, the network can store and execute programs such as ‘for’ loops or ‘if’ or ‘go to’ statements in a certain memory location. This minimal configuration enables the computation of arbitrary algorithms.Furthermore, these activities and plasticity can be read as variational Bayesian inferences under a class of factorial POMDPs, owing to the established equivalence. This enables the deployment of strong variational Bayesian inference tools to explain the characteristics of canonical neural networks implementing Turing machines. The one-to-one correspondence of their components is summarised in Table 1. The formal definition of a Turing machine in terms of a probabilistic generative model is explained in the Methods section. In essence, the dynamics of canonical neural networks can be read as performing a Bayesian inference of external Turing machines, wherein xt, yt, and ω encode the posterior belief about hidden states st, memory readout δt, and parameters θ, respectively. Particularly, when the environment is expressed by deterministic mappings, the states and parameters of canonical neural networks tend to take either 0 or 1 to match those of external milieu as learning progresses. Owing to this property, biological agents can recapitulate algorithms existing in the external milieu and imitate their computations within neural networks in a self-organising manner.Table 1 Correspondence of components of canonical neural networks, Bayesian inference, and turning machinesFull size tableThese propositions were corroborated using numerical simulations, demonstrating that canonical neural networks can infer the computational processes of foundational algorithms, such as adders, based on noisy observations (Fig. 2a). Here, the external Turing machine comprises hidden states (st) and memory (C), which involve the information of 16-dimensional binary numbers and their sum, respectively. The sensory inputs (ot) involved sets of MNIST handwritten digits (0 or 1) representing 16-digit binary numbers that were generated based on st. Learning continued for four sessions (2048 steps).Fig. 2: Canonical neural network implementation of adders.a Schematic of interaction between external adder and canonical neural network. The environment comprises sensory inputs (ot), hidden states (st), readout (δt), header position (\({\psi }_{t}^{\delta }\)), writing (γt), memory (C), and automaton parameters (θ) that encode transition mappings. Sensory inputs comprise a sequence of handwritten 16-digit binary (i.e., 16-bit) numbers and a 16-step-cycle counter to count the passage of time. An example sensory input (handwritten digit) pattern is shown on the top. Each handwritten digit comprises a 28 × 28 image taking values of 0 or 1, and the counter input comprises 256 states to represent 16 steps. Additionally, the agent’s mental action is introduced as input. Thus, sensory inputs are 12,801-dimensional binary vectors. The binary numbers encoded in the hidden states are added in the memory. The canonical neural network comprises the recurrent neural network (xt) and output layer (yt). b Transition of neural activity (xt) that encodes the hidden state posterior (st). Hidden states comprise a 32-dimensional binary vector, in which half the states (1–16) correspond to 16-digit binary numbers and the other half (17–32) represent the counter that iterates 16 step sequences. The neural activity (xt) matches the true hidden states (st), indicating the success of canonical neural networks performing Bayesian inference of external Turing machine’s states. c Transition of synaptic weights (V) that encode the posterior expectation of memory states (C). The values of V express the summation of numbers encoded by hidden states. Please refer to the Methods section for details of the writing rules. d Matching between true and estimated binary numbers. In the latter half of training, the posterior expectation of the memory is in good agreement with the external memory values. e Total number estimation error decreasing with time. f Error in estimating parameters of external adders. The estimation error of the B matrix decreases continuously with time, and that of the memory (C) decreases with sessions. Hence, canonical neural networks could infer the external adder’s states in a self-organising manner. In (e and f), lines and shaded areas represent median values and areas between the first and third quartiles obtained from 100 simulations.Full size imageOn receiving these inputs, the canonical neural networks were able to infer hidden states (Fig. 2b) and add the binary numbers iteratively to output-layer synaptic weights V (Fig. 2c). Rapid plasticity modulated by the risk Γt enabled V to function as the internal memory inferring the external memory states. This is analogous to Bayesian inference of higher layer hidden states in hierarchical models. The matching between the numbers encoded in C and V indicates the success of the Bayesian inference of the hidden states and memory of the external Turing machine (Fig. 2d). Over sessions, the error in imitating the memory dynamics of the external Turing machine was decreased (Fig. 2e). Moreover, the networks could learn the parameters of the external Turing machines (Fig. 2f), thereby recapitulating their programs within the network’s synaptic weights. Therefore, both canonical neural networks and Turing machines can be configured such that they perform the summation of binary numbers. Although the risk or memory-writing rule was given a priori in these simulations, the next section addresses the emergent mechanisms of risk through natural selection.In summary, the equivalence between canonical neural networks, variational Bayesian inference, and differentiable Turing machines was established. The dynamics of canonical neural networks can be read as Bayesian belief updates that entail the agent’s internal states encoding posterior beliefs about the external Turing machine. These results imply the intrinsic universality of canonical neural networks with mental actions and fast modulated plasticity, enabling the inference of external algorithms in a biologically plausible manner. However, these inferences rely on genetically encoded generative models, in which arbitrarily selected genes are suboptimal for a given environment. Thus, the subsequent section addresses the types of algorithms or generative models that emerge evolutionarily through interactions with the environment.Helmholtz energy minimisation derives natural selection and active Bayesian model selectionIn this section, evolution is characterised in terms of Helmholtz energy minimisation and Bayesian model selection (Fig. 3a). Evolution is driven by the differential survival of individuals according to their fitness and survivability, which determine the transmissibility of their distinct genetic blueprints. Each biological agent is encoded by an Nξ-dimensional gene \(\xi \in {\left\{{{\mathrm{0,1}}}\right\}}^{{N}_{\xi }}\), which characterises the network architecture and implicit algorithm, including network size. Natural selection is characterised by the reproduction rate \(\rho \left({o}_{1:T}\right)\) that determines the expected number of offspring per an individual as a function of sensory input sequence o1:T, where T denotes the lifetime duration. Here, the sensory inputs include consequences of the agent’s own actions. The gene distribution of the current generation is denoted as n(ξ), and the offspring distribution is given as$$N\left({o}_{1:T},\xi \right)=\rho \left({o}_{1:T}\right)P\left({o}_{1:T}|\xi \right)n\left(\xi \right)$$
                    (6)
                Fig. 3: Evolutionary emergence of neural networks implementing Turing machines.a Flow diagram of the evolutionary scheme. Natural selection optimises the genes (ξ) that determine the form of the Helmholtz energy, whereas synaptic plasticity (i.e., learning) and neural activity (i.e., inference) update the posterior beliefs about parameters (θ) and autonomous states (st,δt), respectively. Agents consequently engage in actions that minimise the risk associated with future outcomes. These processes optimise across various time scales and hierarchies, all fundamentally based on Helmholtz energy minimisation. b Gene characterises the memory-writing (modulation) rule in canonical neural networks. Three rows represent current states (1–16 elements of xt), previous memory (V), and updated memory (V′). Left: suboptimal gene and corresponding writing rule. Right: optimal gene—namely, ξ = (1,0,0,1,0,1,0,1)T in this setting—and corresponding writing rule, which works as an adder. c Evolution of gene probabilities. Environment is characterised by adders same as in Fig. 2. The evolutionary rule configuration is structured so that the reproduction rate \(\rho \left({o}_{1:t}\right)\) increases as the prediction error of the external memory (i.e., summation of the input numbers) decreases. Probabilities of the optimal gene (red) and several suboptimal genes (black) are shown, The synthetic species started with random writing rules, and only species with appropriate writing rules for an adder survived through natural selection. This explained how the optimal algorithms can emerge in a self-organising manner. d Emergence of neural networks encoding adders. The heat map shows evolutionary change in gene distribution. Each generation consists of 100 agents. The initial condition corresponds to the suboptimal, randomly selected generative model. A cycle of adaptation and evolution facilitates active Bayesian model selection, by which the gene that encodes an optimal generative model is selected. e Errors in estimating memory states. The synaptic strengths (V) encode the posterior belief about memory (C). f Decrease of total number estimation error with the increase in generation. In (e and f), lines and shaded areas represent median values and areas between the first and third quartiles obtained with 100 agents.Full size imageThis indicates that the number of offspring that equips ξ and observes o1:T is determined based on the product of \(\rho \left({o}_{1:T}\right)\), n(ξ), and conditional probability P(o1:T|ξ). In the presence of genetic mutation during natural selection, n(ξ) in Eq. (6) may be replaced with \({n}^{{\prime} }\left(\xi \right)=\nu \left(\xi \right)* n\left(\xi \right)\), where \(\nu \left(\xi \right)*\) indicates the convolution with mutation probability ν(ξ). Equation (6) selects a favourable environment and gene distribution for the next generation, which is the driving force of evolution. The gene is selected more frequently if the corresponding phenotype receives preferable sensory inputs \({o}_{1:T}\) associated with a higher reproduction rate. After sufficient evolution, only genes that maximise the number of offspring survive, resulting in the marginal gene distribution \(N\left(\xi \right)={\sum}_{{o}_{1:T}}N\left({o}_{1:T},\xi \right)\) with sharp peaks at the optimal genes. However, this classical view does not specify the characteristics of the emergent phenotypes. Thus, it is necessary to phenotype the synthetic species (i.e., algorithms) encoded by the selected genes in relation to the given environment.Crucially, natural selection can be derived from variational principles at the species or population level. While each individual minimises the Helmholtz energy \({{{\mathcal{A}}}}\) under a particular \({o}_{1:T}\), the entire species experiences a sufficient number of various \({o}_{1:T}\) generated from the generative process \(P\left({o}_{1:T},\vartheta \right)\), which facilitates the computing of the expectation over \({o}_{1:T}\). By extending Eq. (3), the ensemble Helmholtz energy for the entire species is defined as follows:$$\bar{{{{\mathcal{A}}}}}\left[\pi \left({o}_{1:T},\xi \right)\right]={\left\langle {{{\mathcal{A}}}}\left[\pi \left(\varphi \right),{o}_{1:T},\xi \right]-{{{\mathrm{ln}}}}\,\,\rho \left({o}_{1:T}\right)-{{{\mathrm{ln}}}}\,\,n\left(\xi \right) +{{{\mathrm{ln}}}}\,\,\pi \left({o}_{1:T},\xi \right)\right\rangle }_{\pi \left({o}_{1:T},\xi \right)}$$
                    (7)
                where \(\pi \left({o}_{1:T},\xi \right)\) denotes the joint distribution of \({o}_{1:T}\) and ξ for the offspring generation and \({\left\langle \cdot \right\rangle }_{\pi \left({o}_{1:T},\xi \right)}\) is the expectation over \(\pi \left({o}_{1:T},\xi \right)\). During the lifetime, each synthetic species optimises π(φ) by interacting with the environment, followed by determination of the offspring distribution \(\pi \left({o}_{1:T},\xi \right)\), each of which can be read as an individual adaptation and natural selection, respectively. As noted above, \({{{\mathcal{A}}}}\left[\pi \left(\varphi \right),{o}_{1:T},\xi \right]\) converges to \(-{{{\mathrm{ln}}}}\,{P}_{m}\left({o}_{1:T}\right)\) in the steady states. From the variational method, solving the fixed point \(\delta \bar{{{{\mathcal{A}}}}}=0\) with respect to \(\pi \left({o}_{1:T},\xi \right)\)—after the convergence of π(φ) to the steady state—yields the offspring distribution:$$\pi \left({o}_{1:T},\xi \right)=\frac{1}{Z}\rho \left({o}_{1:T}\right){P}_{m}\left({o}_{1:T}\right)n\left(\xi \right)$$
                    (8)
                where \(Z={\sum}_{{o}_{1:T},\xi }\rho \left({o}_{1:T}\right){P}_{m}\left({o}_{1:T}\right)n\left(\xi \right)\) denotes the partition function. When genetically encoded generative models have sufficient representation capacity, the marginal likelihood \({P}_{m}\left({o}_{1:T}\right)\) can be viewed as a good approximation of the conditional likelihood \(P\left({o}_{1:T}{|m}\right)\). Moreover, because ξ encodes model structure m = m(ξ), \(P\left({o}_{1:T}{|m}\right)=P\left({o}_{1:T}|\xi \right)\) holds by construction. Thus, Eq. (8) is asymptotically equivalent to the offspring distribution derived from natural selection (Eq. (6)) up to the normalisation. Moreover, Z formally corresponds to the total number of offspring, which is consistent with previous work42,43,44. Hence, although each individual does not know \(\rho \left({o}_{1:T}\right)\) or n(ξ) directly, the variational principle enables the optimisation of \(\pi \left({o}_{1:T},\xi \right)\) through the interaction between the species and the environment. Further details are provided in the Methods section.According to the equivalence19,20,21, a variational free energy \(\bar{{{{\mathcal{F}}}}}\) corresponding to \(\bar{{{{\mathcal{A}}}}}\) exists, which yields the inequality of evolution:$$\bar{{{{\mathcal{A}}}}}\left[\pi \left({o}_{1:T},\xi \right)\right]\equiv \bar{{{{\mathcal{F}}}}}\left[Q\left({o}_{1:T},m\right)\right]\ge -{{{\mathrm{ln}}}}\,Z$$
                    (9)
                This indicates that \(\bar{{{{\mathcal{A}}}}}\) upper bounds the negative logarithm of the total offspring number Z. The equality holds at the steady state, which is obtained by substituting Eq. (8) into Eq. (7). Emergent synthetic species can yield adaptive sentient behaviours that maximise the reproduction rate. When the gene mutation probability is sufficiently small, agents with other suboptimal genes—which have biased generative modes and lower survival and reproduction probabilities—are asymptotically eliminated after sufficient evolution. This results in the emergence of sharp peaks in gene distribution. Variations in ξ may exist if these are unrelated to optimisation. Another gene distribution may appear when the gene mutation probability is large45.It is crucial that Eq. (9) also holds true for the variational free energy \(\bar{{{{\mathcal{F}}}}}\), where \(Q\left({o}_{1:T},m\right)\) denotes the posterior belief about the offspring’s observations and genes. Minimising \(\bar{{{{\mathcal{F}}}}}\) (\(\equiv \bar{{{{\mathcal{A}}}}}\)) furnishes an active Bayesian model selection that selects desirable sensory inputs \({o}_{1:t}\) for the next generation. An optimal gene that minimises \(\bar{{{{\mathcal{A}}}}}\equiv \bar{{{{\mathcal{F}}}}}\) simultaneously minimises −ln Z, by which the agent attains the generative model \({P}_{m}\left({o}_{1:T},\vartheta \right)\) that best recapitulates the true generative process of external milieu \(P\left({o}_{1:T},\vartheta \right)\), known as Hamiltonian matching21. This occurs because the offspring number is maximised most efficiently—and thus the gene distribution reaches a steady state—when \({P}_{m}\left({o}_{1:T},\vartheta \right)\) best matches \(P\left({o}_{1:T},\vartheta \right)\). Hence, natural selection derived from the Helmholtz energy minimisation acts as active Bayesian model selection that optimises the generative model of agents. These observations are consistent with the outcomes in previous works that applied the free-energy principle to the explanation of evolutionary systems46,47,48,49.In particular, when the external milieu comprises the considered class of Turing machines, an optimal generative model is a canonical neural network. Thus, ensemble Helmholtz energy minimisation over generations makes synthetic species have evolved to equip canonical neural networks. These networks then make Bayesian inference of the states of external Turing machines in a self-organising manner.These propositions were corroborated by numerical simulations of evolution. The external milieu was characterised by adders considered in Fig. 2. Biological agents were formulated by Eq. (1), and risks or memory-writing rules were characterised by the genes (ξ). The simulation comprised a cycle of adaptation and evolution (Fig. 3a). The genes were updated through genetic mutation at the natural selection stage, in which agents with a higher accuracy of estimating the summation of input numbers exhibited a greater reproduction rate.At the beginning of evolution, neural networks employed the Helmholtz energy characterised by randomly selected genes, which derived suboptimal algorithms or strategies. Their writing rules were distinct from that for adders (Fig. 3b, left), yielding large prediction errors and thus small reproduction rates. However, as no agent had an optimal gene in the first few generations, better phenotypes among them resulted in relatively higher reproduction (Fig. 3c). This evolutionary process selects a generative model with risk function that maximise reproduction, which led to a gradual convergence to the optimal gene that expressed the writing rule of an adder (Fig. 3b, right). Thus, the limitation of the writing rule to be specified in Fig. 2 was relaxed in Fig. 3. As expected, only species with the optimal gene survived after sufficient generations (Fig. 3c), and the final gene distribution exhibited a sharp peak at the optimal gene that encodes an apt generative model (Fig. 3d). These synthetic species evolved to exhibit Bayesian belief updating under the optimal (i.e., unbiased) prior belief and yielded superior performance compared to other species with suboptimal genes. This was confirmed by monotonic decrease of memory state estimation errors over generations (Fig. 3e). Consequently, the surviving species could implement an adder in their network architecture and estimate the summation of input binary numbers (Fig. 3f).Therefore, despite of no direct feedback to the environmental states, the mental actions of agents influence the environment through evolution by changing the reproduction rate and sensory input distribution in the subsequent generations. This can be read as active Bayesian model selection that actively interfere with the external environment. As a result, individuals that exhibit appropriate mental actions as adders are naturally selected. Note that agents can in general exhibit actions that directly change the environment.In summary, ensemble Helmholtz energy minimisation can be used to derive natural selection that maximises the total number of offspring. The natural selection functions as active Bayesian model selection, which facilitates emergent species to best recapitulate the given environment and exhibit Bayes-optimal decision making. This notion renders evolution explainable in terms of Bayesian inference based on generative models, facilitating the universal characterisation of biological intelligence emerging from evolution.Canonical neural networks can implement universal Turing machinesFinally, the possibility of canonical neural networks becoming universal machines is addressed. Some Turing machines can emulate algorithms implemented by other Turing machines within a single architecture by storing programs in memory. These are referred to as universal Turing machines1, and can be constructed using a small circuit, such as that with Nx = 2 states and 18 memory values50 or Nx = 18 states and 2 memory values51. This implies that even relatively small neural networks can function as universal machines.These universal machines can be straightforwardly implemented using a POMPD with two headers and memories (i.e., two mental actions). In the POMDP formation, the state transition is determined based on a set of previous states (st−1), memory readout (δt−1), and transition matrix (B), as \({s}_{t}=B\left({s}_{t-1}\otimes {\delta }_{t-1}\right)\) using a Kronecker product. This constructs an algorithm. Conversely, the product of new \({\delta }_{t-1}^{{\prime} }\) and B′ (⋅) can be used to construct transition mapping \({B}^{{\prime} }\left({\delta }_{t-1}^{{\prime} }\right)\) that follows the program encoded in \({\delta }_{t-1}^{{\prime} }\). Then, with the second memory readout δt-1, transition mapping \({s}_{t}={B}^{{\prime} }\left({\delta }_{t-1}^{{\prime} }\right)\left({s}_{t-1}\otimes {\delta }_{t-1}\right)\) is constructed, with which agents can emulate the aforementioned algorithm when \({\delta }_{t-1}^{{\prime} }\) satisfies \({B}^{{\prime} }\left({\delta }_{t-1}^{{\prime} }\right)=B\). This is a construction of a universal Turing machine, which enables the computation of arbitrary transitions of st according to \({\delta }_{t}^{{\prime} }\) and δt.Owing to the equivalence, canonical neural networks with two mental actions can implement universal Turing machines in a straightforward manner. Diverse programs can be executed with a single neural network by reading them from the output-layer synaptic weights. Using this architecture, canonical neural networks can learn to infer the states of multiple Turing machines in the environment. Selecting an appropriate transition mapping by moving a header position is associated with an attentional switch41,52, which may be encoded by neuromodulators such as dopaminergic neurons53.The simulation environment included 10 algorithms or machines, with each comprising 10 states (\({s}_{t}\in {\left\{{{\mathrm{0,1}}}\right\}}^{10}\)) and a memory of length 10 (\(C\in {\left\{{{\mathrm{0,1}}}\right\}}^{1\times 10}\)) as well as randomly generated transition rules (\(B\in {\left\{{{\mathrm{0,1}}}\right\}}^{10\times 10\times 2}\)) that differ from each other (Fig. 4a). Agents could observe sensory inputs \({o}_{t}\in {\left\{{{\mathrm{0,1}}}\right\}}^{10}\) generated by st, whereas the other variables were unobservable. For simplicity, in this simulation, the agents and external algorithms were considered to share common memory writing rules (Γt) and likelihood mapping (A). In this environment, the algorithms were switched at random intervals. Thus, agents were required to infer which algorithm was generating the current input and what transition rules it employed.Fig. 4: Neuronal implementations of universal machines.a Schematic of the interaction between 10 external machines and a canonical neural network involving two mental actions. b Transition of the basis function (\({\psi }_{t}^{{y}^{{\prime} }}\)) that encodes the reading header position. After training, the network accurately infers which machine is generating the sensory inputs. c Errors in estimating the transition matrices (B) of the external machines. Ten colour lines indicate estimation errors for 10 machines in a simulation. d Errors in estimating the transition matrices over 20 simulations. e Errors in inferring which machine is generating the input. The results indicate that the reading header (\({\psi }_{t}^{{y}^{{\prime} }}\)) moves to the correct position after learning. f Errors in predicting subsequent hidden or automaton states (st+1) of the external machines. In (d–f), lines and shaded areas represent median values and areas between the first and third quartiles obtained from 20 simulations.Full size imageThe simulation results demonstrated that canonical neural networks successfully distinguished among 10 different external algorithms by accumulating evidence (Fig. 4b) and predicted their dynamics by learning and storing their transition matrices in the memory. The header of the memory location corresponds to the model with the highest likelihood, analogous to the mixture generative model41 and neural Turing machine26,27 proposed in previous works. During a 105-time-step session, canonical neural networks successfully learned to encode 10 transition matrices within synaptic weights V (Fig. 4c). This was reliably observed in 20 simulations with different environmental settings (Fig. 4d). Importantly, because these transition matrices are stored individually in V, the agents allow to the reuse of the transition matrices learned in the past to make rapid predictions when receiving sensory inputs from previously experienced algorithms. Owing to this, canonical neural networks could move the header to an appropriate memory location, with small estimation error (Fig. 4e). Accordingly, errors in predicting the subsequent hidden states decreased with time (Fig. 4f).In summary, canonical neural networks can biologically plausibly implement universal Turing machines using two mental actions and fast modulated synaptic plasticity. These characteristics are crucial for developing a flexible and adaptable biological intelligence. It is the virtue of canonical neural networks that they can in principle emulate arbitrary algorithms owing to suggested Turing completeness or computational universality1.

## Discussion
Most computational models in neuroscience have employed neural networks specialised for a specific task. Although previous works investigated artificial neural networks with external memory26,27 and neuronal implementations of Turing machines28,29,30, the emergent mechanics of Turing machines is a more delicate problem. To address these limitations, this work constructed canonical neural networks that could autonomously acquire diverse algorithms by performing Bayesian inference of external Turing machines. Their update rules naturally emerge as a corollary of Helmholtz energy minimisation and implicit Bayesian belief updates. This is distinct from conventional neural networks that only employ a fixed generative model or finite-state machine without tape, which can only perform a specific hard-coded program. Therefore, canonical neural networks are potentially important for modelling generic intelligence beyond these limitations.Mental action and modulated plasticity enable the implementation of memory reading and writing, which is essential for canonical neural networks to function as a Turing machine. Through mental actions, these networks update synaptic weights that infer the external memory states and learn the transition mappings in a self-organising manner. This process can be viewed as a generalised active inference that actively changes their internal memory states. By doing so, canonical neural networks perform a Bayesian inference of the external automaton and memory states. Brain regions with sparse information representation and high plasticity, such as the hippocampus54, may play the role of memory (i.e., the output-layer synaptic weights \(V\)). For instance, behavioural time scale synaptic plasticity in the hippocampus occurs on a timescale of seconds and changes synaptic weights significantly in a single event, enabling one-shot learning of place fields55,56. Such fast plasticity may be able to implement memory writing in \(V\).Genes that encode programs or generative models are shaped through natural selection. Characterising the associated algorithms is essential for understanding biological intelligence. Owing to the correspondence between natural selection and active Bayesian model selection, biological agents evolve to perform active inference of the external milieu states under an apt generative model. This is attributed to the complete class theorem34,35,36, which guarantees that algorithms that minimise cost functions—in the sense of admissible rules—can be read as performing Bayesian inferences. This notion lends the explainability of Bayesian inference to the characterisation of generic biological agents (i.e., algorithms) and evolution.The synthetic species follow an asymptotic trajectory to identify generative models that best recapitulate environmental generative processes in their internal states. This notion describes the emergent mechanisms of surprise minimisation, generative models, and preference priors in neural networks—which are provided as a priori assumptions in the current formulation of the free-energy principle17,18,38,39,40—from an evolutionary perspective. The generative model implemented in the network architecture becomes accurate through natural selection, because it most effectively minimises the Helmholtz energy and thus maximises their chances of reproduction. These optimisations may be accelerated by combining local gradient descent with genetic algorithms57.In the considered setting, time progresses continuously as the network processes the signals. Therefore, if the process exceeds the lifetime duration, agents with suboptimal algorithms cannot produce efficient outcomes, and would be eliminated during natural selection. Thus, the surviving species may efficiently avoid the so-called hold problem. One can expect the emergence of efficient search in the algorithmic space that can easily use previous constructs to build new ones recursively within canonical neural networks. Existing mappings between Turing machines and lambda calculus may be useful in understanding architectural improvements that allow for increasingly complex computations, which is an interesting direction for future work.Generally, the external milieu comprises an environment and multiple agents that adapt and evolve, prompting us to consider the coevolution of these agents. Generic dynamical systems, including multiple agents, can be described by interactions between Turing machines. A possible extension of the proposed framework to the coevolution of multiple agents is considered in the Methods section, although details are not provided in this paper. The analyses suggest that the coevolution of multiple agents entails the emergence of generalised synchrony in agents’ internal states and strategies58,59, wherein each agent infers the algorithms of other agents. The Bayesian perspective suggests that some psychiatric disorders are caused by a break in balance in the integration of sensory information with prior beliefs60. Mutual inferences of Turing machines could be related to the model of psychiatric disorders associated with attenuated social intelligence. In the presence of social interactions and coevolution, the complexity of the generative model tends to increase because each agent evolves to infer and imitate algorithms of other agents. These processes lead to chaotic progressions in which the final states can vary significantly. Because of these difficulties, formal characterisations of social interaction and coevolution are beyond the scope of this paper, which is an important research direction for future work.In summary, the triple equivalence between canonical neural networks, Bayesian inference, and Turing machines was demonstrated. Canonical neural networks with mental actions and modulated plasticity can implement a Turing machine in a biologically plausible manner and perform variational Bayesian inferences of external Turing machines. The Helmholtz energy minimisation at the species level involves natural selection. An appropriate network architecture (i.e., generative model) naturally emerge through active Bayesian model selection, which is implicit in natural selection. These notions offer a universal characterisation of biological intelligence emerging naturally from evolution, providing a possible explanation for the emergence of generic biological intelligence and an insight into the development of artificial general intelligence.

## Methods
Turing machines in the form of POMDP generative modelsIn this work, the environment is defined as a Turing machine expressed in the form of a factorial POMDP. The Turing machine comprises internal states of automaton \({s}_{t}{{{\mathcal{\in }}}}{{{\mathcal{S}}}}\), memory \(C\), readout from the memory \({\delta }_{t}{{{\mathcal{\in }}}}{{{\mathcal{D}}}}\), header position \({\psi }_{t}^{\delta }{{{\mathcal{\in }}}}{{{\mathcal{M}}}}\), memory writing \({\gamma }_{t}{{{\mathcal{\in }}}}{{{\mathcal{W}}}}\), and transition mapping \({{{\mathcal{S}}}}{{{\mathcal{\times }}}}{{{\mathcal{D}}}}\,{{{\mathcal{\longmapsto }}}}\,{{{\mathcal{S}}}}{{{\mathcal{\times }}}}{{{\mathcal{W}}}}{{{\mathcal{\times }}}}{{{\mathcal{M}}}}\). The initial and final (accepting) states are denoted as \({s}_{1}\) and \({s}_{{acc}}\), respectively. Memory \(C\) has a finite but sufficient length, and its elements usually take a binary value. In terms of the generative model, Eq. (5) is characterised by categorical distributions, from which binary vector variables \({s}_{t}\), \({\delta }_{t}\), and \({\gamma }_{t}\) are sampled. The POMDP becomes a deterministic Turing machine with deterministic transition mappings, whereas it becomes a stochastic Turing machine when state transitions involve some stochasticity.The automaton updates the hidden states \({s}_{t}\) following \(P\left({s}_{t}|{s}_{t-1},{\delta }_{t-1},B\right)={{{\rm{Cat}}}}\left(B{\psi }_{t-1}^{s}\right)\), where \({\psi }_{t}^{s}=\psi \left({x}_{t},{y}_{t}\right)\) denotes basis functions which is an arbitrary function of \({x}_{t}\) and \({y}_{t}\); for example, it is defined as a Kronecker product of \({s}_{t}\) and \({\delta }_{t}\), \({\psi }_{t}^{s}={s}_{t}\otimes {\delta }_{t}\). Then, it generates (mental) actions \({\delta }_{t}\) depending on the policy or memory matrix \(C\), in which \({\delta }_{t}\) formally corresponds to the memory readout that reads the memory information at position \({\psi }_{t-1}^{\delta }\). This is given as \(P\left({\delta }_{t}|{s}_{t-1},{\delta }_{t-1},C\right)={{{\rm{Cat}}}}\left(C{\psi }_{t-1}^{\delta }\right)\) using basis function or header position. The reading header position \({\psi }_{t}^{\delta }={\psi }^{\delta }\left({s}_{t},{\delta }_{t},{\psi }_{t-1}^{\delta }\right)\) moves depending on \({s}_{t},{\delta }_{t-1}\), and \({\psi }_{t-1}^{\delta }\). State basis \({\psi }_{t}^{s}\) and reading header position \({\psi }_{t}^{\delta }\) are treated as dependent variables. The Turing machine further writes the new information to the memory at the current header position depending on risk \({\gamma }_{t}\), which follows \(P\left({\gamma }_{{ti}}=1\right)={\varGamma }_{{ti}}\), where \({\varGamma }_{t}=\varGamma \left({s}_{t},{\delta }_{t},{o}_{t}\right)\). In the writing phase, \(C{\psi }_{t-1}^{\delta }\) is retained when \({\gamma }_{t}=0\) or inverted when \({\gamma }_{t}=1\). The output of the Turing machine becomes the sensory input \({o}_{t}\) for biological agents: \(P\left({o}_{t}|{s}_{t},A\right)={{{\rm{Cat}}}}\left(A{s}_{t}\right)\). The sensory input can be viewed as a readout from a tape shared by the environment and agent.Therefore, the generative model is defined as follows:$${P}_{m}\left({o}_{1:t},{s}_{1:t},{\delta }_{1:t},\theta ,\lambda \right)=P\left(\theta \right)P\left(\lambda \right){\prod }_{\tau =1}^{t}P\left({o}_{\tau }|{s}_{\tau },A\right)P\left({s}_{\tau }|{s}_{\tau -1},{\delta }_{\tau -1},B\right)P\left({\delta }_{\tau }|{s}_{\tau -1},{\delta }_{\tau -1},{\gamma }_{\tau },C\right)$$
                    (10)
                where θ = {A, B, C} denotes a set of parameters and \(\lambda =\left\{D,E\right\}\) denotes a set of hyper parameters. Components of \(P\left(\theta \right)P\left(\lambda \right)=P\left(A\right)P\left(B\right)P\left(C\right)P\left(D\right)P\left(E\right)\) follow Dirichlet distributions. These parameters have prior beliefs that persist with the values learned in the past, whereas \(C\) changes quickly compared to other parameters owing to an effectively small Dirichlet count. To minimise the risk, the \(C\)’s posterior belief is updated using a counterfactual generative model defined previously20, in which \(P\left({\delta }_{\tau }|{s}_{\tau -1},{\delta }_{\tau -1},{\gamma }_{\tau },C\right)={{{\rm{Cat}}}}\left(C{\psi }_{\tau -1}^{\delta }\right)\) for \({\gamma }_{\tau }=0\) while \(P\left({\delta }_{\tau }|{s}_{\tau -1},{\delta }_{\tau -1},{\gamma }_{\tau },C\right)\propto {{{\rm{Cat}}}}\left({C}^{\odot -1}{\psi }_{\tau -1}^{\delta }\right)\) for \({\gamma }_{\tau }=1\) using the Hadamard power of \(C\). The initial values of \(C\) may follow a flat prior of 0.5, which expresses a white space character. Note that POMPDs with a factorial structure are considered herein but a simplified notation is adopted; for more details, please refer to previous work19,20. A set of external milieu variables is denoted as \(\vartheta =\left\{{s}_{1:t},{\delta }_{1:t},\theta ,\lambda \right\}\). The generative model \({P}_{m}\left({o}_{1:t},\vartheta \right)\) may or may not be equal to the true generative process of the external milieu, \(P\left({o}_{1:t},\vartheta \right)\), where model structure \(m\) is optimised during the evolution.In essence, the considered class of POMDPs is cast as Turing machines when the \(C\) matrix can be read as a sufficiently long tape (or memory), in which the column index of \(C\) corresponds to the memory address. A sequential-access header \({\psi }_{t}^{\delta }\) only moves to the right or left cell from the current position, which is sufficient for Turing machines. However, random-access header \({\psi }_{t}^{\delta }\) that freely moves to any address location is more useful for practical configurations, as considered in previous works26,27. Generally, multiple actions \({\delta }_{t}\) may separately express memory readout (i.e., mental action) and direct feedback responses to the environment.Variational free energy minimisationVariational free energy (Eq. (4)) provides an upper bound of the surprise \(-{{{\mathrm{ln}}}}\,{P}_{m}\left({o}_{1:t}\right)\). Solving the fixed point of the \({{{\mathcal{F}}}}\)’s variation \(\delta {{{\mathcal{F}}}}=0\) provides approximate posterior belief \(Q\left(\vartheta \right)\), which minimises \({{{\mathcal{F}}}}\) and results in \({{{\mathcal{F}}}} \simeq - {{{\mathrm{ln}}}}\,{P}_{m}\left({o}_{1:t}\right)\). When the environment is expressed as POMDPs, posterior expectation \({{{\boldsymbol{\vartheta }}}}:= {{{{\rm{E}}}}}_{Q\left(\vartheta \right)}\left[\vartheta \right]\) or its counterpart is sufficient to approximate \(Q\left(\vartheta \right)\). Thus, \({{{\mathcal{F}}}}\) is reduced to a function of \({{{\boldsymbol{\vartheta }}}}\), \(F\left(o,{{{\boldsymbol{\vartheta }}}}\right)\). Throughout the paper, bold case variables (e.g., \({{{\boldsymbol{\vartheta }}}}\)) are used to indicate the posterior expectation of the corresponding random variable (e.g., \(\vartheta\)).In this setting, an approximate posterior belief is given as follows:$$Q({s}_{1:t},{\delta }_{1:t},\theta )=Q(A)Q(B)Q(C){\prod }_{\tau =1}^{t}Q({s}_{\tau })Q({\delta }_{\tau })$$
                    (11)
                where Q(sτ) = Cat(sτ) and Q(δτ) = Cat(δτ) are categorical distributions and Q(A) = Dir(a), Q(B) = Dir(b), and Q(C) = Dir(η−1(c)) are Dirichlet distributions. Due to the factorial nature, these posterior expectations comprise the outer products of submatrices19,20. For Q(C), nonlinear function η−1(∙) is composed so that the values of c restricted within the range of 0 and 1. For simplicity, D and E are treated as fixed parameters. By substituting Eqs. (10) and (11) into Eq. (4), the variational free energy for the considered POMDPs is obtained as follows:$${\mathcal F} = 	{\sum }_{\tau =1}^{t}{{{{\bf{s}}}}}_{\tau }\cdot \{{{{\mathrm{ln}}}}\,{{{{\bf{s}}}}}_{\tau }-\,{{{\mathrm{ln}}}}\,{{{\bf{A}}}}\cdot {o}_{\tau }-\,{{{\mathrm{ln}}}}\,{{{\bf{B}}}}{{{{\mathbf{\psi }}}}}_{\tau -1}^{s}\} \\ 	+{\sum }_{\tau =1}^{t}{{{{\mathbf{\delta }}}}}_{\tau }\cdot \{{{{\mathrm{ln}}}}\,{{{{\mathbf{\delta }}}}}_{\tau }-(\overrightarrow{1}-2{\varGamma }_{\tau })\odot \,{{{\mathrm{ln}}}}\,{{{\bf{C}}}}{{{{\mathbf{\psi }}}}}_{\tau -1}^{\delta }\}+{{{{\mathcal{D}}}}}_{{{{\rm{KL}}}}}[Q(\theta )\parallel P(\theta )]$$
                    (12)
                Here, the Kullback–Leibler divergence \({{{{\mathcal{D}}}}}_{{{{\rm{KL}}}}}[Q(\theta )\parallel P(\theta )]\) represents the complexity of the parameters, which is in the order of ln t and thus negligibly smaller than the leading order term. When written explicitly, it is given as \({{{{\mathcal{D}}}}}_{{{{\rm{KL}}}}}\left[Q\left(\theta \right){||P}\left(\theta \right)\right]={{{{\mathcal{D}}}}}_{{{{\rm{KL}}}}}\left[Q\left(A\right){||P}\left(A\right)\right]+{{{{\mathcal{D}}}}}_{{{{\rm{KL}}}}}\left[Q\left(B\right){||P}\left(B\right)\right]+{{{{\mathcal{D}}}}}_{{{{\rm{KL}}}}}\left[Q\left(C\right){||P}\left(C\right)\right]\).Each element of ϑ is optimised following the gradient descent on \({{{\mathcal{F}}}}\), \(\dot{{{{\boldsymbol{\vartheta }}}}}\propto -{\partial }_{{{{\boldsymbol{\vartheta }}}}}{{{\mathcal{F}}}}\). In short, the hidden states (st) and memory (C) can be associated with the lower and higher layers of a hierarchical generative model, and these states can be inferred by accumulating evidence from sensory inputs.The posterior expectations about the hidden states and actions—in the form of a Bayesian filter—are derived as the fixed-point solution of implicit gradient descent \({\partial }_{{{{{\bf{s}}}}}_{t}}{{{\mathcal{F}}}}=0\) and \({\partial }_{{{{{\mathbf{\delta }}}}}_{t}}{{{\mathcal{F}}}}=0\), which are given as follows:$$\left\{\begin{array}{c}{{{{\bf{s}}}}}_{t}=\sigma \left({{{\mathrm{ln}}}}\,\,{{{\bf{A}}}}\cdot {o}_{t}+{{{\mathrm{ln}}}}\,\,{{{\bf{B}}}}{{{{\mathbf{\psi }}}}}_{t-1}^{s}\right)\\ {{{{\mathbf{\delta }}}}}_{t}=\sigma \left({{\mathrm{ln}}}\,{{{\bf{C}}}}{{{{\mathbf{\psi }}}}}_{t-1}^{\delta }\right)\end{array}\right.$$
                    (13)
                where σ denotes the soft-max function. The updates of st and δt represent state transition and memory readout of the internal Turing machine, respectively. Because st is updated depending on the sensory input term ln A·ot, this configuration makes the internal Turing machine’s states (st) encode the external Turing machine’s states (st). Moreover, these update rules are a family of Eq. (1) and thus formally associated with the neural activity, as described in the subsequent section.Moreover, from \({\partial }_{\theta } {\mathcal F} =O\), the posterior expectations about memory and parameters are updated as follows:$$\left\{\begin{array}{c}{{{\bf{a}}}}\leftarrow {{{\bf{a}}}}+{o}_{t}\otimes {{{{\bf{s}}}}}_{t}\\ {{{\bf{b}}}}\leftarrow {{{\bf{b}}}}+{{{{\bf{s}}}}}_{t}\otimes {{{{\mathbf{\psi }}}}}_{t-1}^{s}\\ {{{\bf{c}}}}\leftarrow \eta \left({{{\bf{c}}}}+\left(\vec{1}-2{\varGamma }_{t}\right)\odot {{{{\mathbf{\delta }}}}}_{t}\otimes {{{{\mathbf{\psi }}}}}_{t-1}^{\delta }\right)\end{array}\right.$$
                    (14)
                The belief update of C implements a memory writing rule, which is expressed as a risk-modulated policy update rule in the form of modulated Hebbian plasticity20. For instance, η(∙) = min(∙−min (∙),1) can be considered to restrict the values of c. This is applied to each column of c. Because c is restricted within a small value, it has a high learning rate and is thus quickly updated. The update of c follows a simple Hebbian rule when the risk is low; thus, the current memory state is retained. Conversely, it becomes an anti-Hebbian rule when the risk is high, where the memory state is flipped. This enables the writing of new information into the memory. Because the value of writing is determined based on past hidden states and actions, the memory can be viewed as a compressed storage of past state/action information.Canonical neural networksIn this section, the equivalence between canonical neural networks and Turing machines is elaborated by extending the natural equivalence between canonical neural networks and variational Bayesian inference under a class of POMDP generative models19,20.Canonical neural networks comprise a two-layer network of rate-coding neurons, where the network’s internal states \(\left\{{x}_{1:t},{y}_{1:t},\omega ,\phi \right\}\) include neural activity ut = {xt,yt}, synaptic weights ω = {W1, W0, K1, K0, V1, V0}, and any other free parameters \(\phi =\left\{{\phi }_{1}^{x},\,{\phi }_{0}^{x},\,{\phi }_{1}^{y},\,{\phi }_{0}^{y}\right\}\) that characterise the Helmholtz energy \({{{\mathcal{A}}}}\). Upon receiving sensory inputs ot, the middle-layer (xt) and output-layer (yt) neural activities are given as follows:$$\left\{\begin{array}{c}{\dot{x}}_{t}\propto -{{{{\rm{sig}}}}}^{-1}\left({x}_{t}\right)+W{o}_{t}+K{\psi }_{t-1}^{x}+{h}^{x}\\ {\dot{y}}_{t}\propto -{{{{\rm{sig}}}}}^{-1}\left({y}_{t}\right)+V{\psi }_{t-1}^{y}+{h}^{y}\end{array}\right.$$
                    (15)
                where \({\psi }_{t}^{x}=\psi \left({x}_{t},{y}_{t}\right)\) and \({\psi }_{t}^{y}=\psi \left({x}_{t},{y}_{t},{\psi }_{t-1}^{y}\right)\) denote basis functions that summarise the middle and output layer neural activity; W = W1–W0, K = K1–K0, and V = V1–V0 are synaptic weights; and \({h}^{x}={h}_{1}^{x}-{h}_{0}^{x}\) and \({h}^{y}={h}_{1}^{y}-{h}_{0}^{y}\) are the adaptive firing thresholds, which are functions of the synaptic weights. One may consider that W1, K1, V1 are excitatory synapses, whereas W0, K0, V0 are inhibitory synapses. The adaptive firing thresholds satisfy \({h}_{l}^{x}={{\mathrm{ln}}}\,{{{\rm{sig}}}}\left({W}_{l}\right)\vec{1}+{{\mathrm{ln}}}\,{{{\rm{sig}}}}\left({K}_{l}\right)\vec{1}+{\phi }_{l}^{x}\) and \({h}_{l}^{y}={{\mathrm{ln}}}\,{{{\rm{sig}}}}\left({V}_{l}\right)\vec{1}+{\phi }_{l}^{y}\) for l = 1,0. Both the middle and output layers involve recurrent circuits with one-time-step delays. The output-layer activity yt determines actions or decisions δt. The fixed point of Eq. (15) provides rate-coding models with a widely used sigmoidal activation function, also known as a neurometric function61.By taking the integral of Eq. (15), a biologically plausible Helmholtz energy for canonical neural networks is obtained19,20:$${{{\mathcal{A}}}}= 	{\sum }_{\tau =1}^{t}{\left(\begin{array}{c}{x}_{\tau }\\ \overline{{x}_{\tau }}\end{array}\right)}^{{{{\rm{T}}}}}\left\{{{{\mathrm{ln}}}}\,\left(\begin{array}{c}{x}_{\tau }\\ \overline{{x}_{\tau }}\end{array}\right)-\left(\begin{array}{c}{W}_{1}\\ {W}_{0}\end{array}\right){o}_{\tau }-\left(\begin{array}{c}{K}_{1}\\ {K}_{0}\end{array}\right){\psi }_{\tau -1}^{x}-\left(\begin{array}{c}{h}_{1}^{x}\\ {h}_{0}^{x}\end{array}\right)\right\} \\ 	+{\sum }_{\tau =1}^{t}{\left(\begin{array}{c}{y}_{\tau }\\ \overline{{y}_{\tau }}\end{array}\right)}^{{{{\rm{T}}}}}\left\{{{{\mathrm{ln}}}}\,\left(\begin{array}{c}{y}_{\tau }\\ \overline{{y}_{\tau }}\end{array}\right)-\left(\begin{array}{c}\vec{1}-2{\varGamma }_{\tau }\\ \vec{1}-2{\varGamma }_{\tau }\end{array}\right)\odot \left(\begin{array}{c}{V}_{\tau 1}\\ {V}_{\tau 0}\end{array}\right){\psi }_{\tau -1}^{y}-\left(\begin{array}{c}{h}_{1}^{y}\\ {h}_{0}^{y}\end{array}\right)\right\}{{{\mathcal{+}}}}{{{\mathcal{C}}}}$$
                    (16)
                where Γt denotes the risk associated with future outcomes. Here, Γt is considered to modulate Hebbian plasticity without delay due to high plasticity rate. Canonical neural networks are characterised by Hamiltonian \({{{{\mathcal{H}}}}}_{\xi }\) encoded by gene ξ 21. From Eq. (16), the expectation of \({{{{\mathcal{H}}}}}_{\xi }\) is given as:$$\left\langle {{{{\mathcal{H}}}}}_{\xi }\right\rangle =	-{\sum }_{\tau =1}^{t}\left[{\left(\begin{array}{c}{x}_{\tau }\\ \overline{{x}_{\tau }}\end{array}\right)}^{{{{\rm{T}}}}}\left\{\left(\begin{array}{c}{W}_{1}\\ {W}_{0}\end{array}\right){o}_{\tau }+\left(\begin{array}{c}{K}_{1}\\ {K}_{0}\end{array}\right){{\psi }}_{\tau -1}^{x}+\left(\begin{array}{c}{h}_{1}^{x}\\ {h}_{0}^{x}\end{array}\right)\right\}\right. \\ 	 \left.+{\left(\begin{array}{c}{y}_{\tau }\\ \overline{{y}_{\tau }}\end{array}\right)}^{{{{\rm{T}}}}}\left\{\left(\begin{array}{c}\vec{1}-2{\varGamma }_{\tau }\\ \vec{1}-2{\varGamma }_{\tau }\end{array}\right)\odot \left(\begin{array}{c}{V}_{\tau 1}\\ {V}_{\tau 0}\end{array}\right){{\psi }}_{\tau -1}^{y}+\left(\begin{array}{c}{h}_{1}^{y}\\ {h}_{0}^{y}\end{array}\right)\right\}\right]+{{\mathcal{O}}}(1)$$
                    (17)
                When neural spikes follow Bernoulli distributions, substituting Eq. (17) into Eq. (2) yields Eq. (16). Here, one can associate \({e}^{-\beta {{{{\mathcal{H}}}}}_{\xi }}\) with the probability distribution of spiking neural activity patterns.Minimising \({{{\mathcal{A}}}}\) furnishes the dynamics of neural networks, including their activity and plasticity. The gradient descent on \({{{\mathcal{A}}}}\) with respect to xt and yt, i.e., \({\dot{x}}_{t}\propto -{\partial }_{{x}_{t}}{{{\mathcal{A}}}}\) and \({\dot{y}}_{t}\propto -{\partial }_{{y}_{t}}{{{\mathcal{A}}}}\), derives Eq. (15) by construction. Moreover, the gradient descent on \({{{\mathcal{A}}}}\) with respect to synaptic weights \(\left\{{W}_{1},{W}_{0},{K}_{1},{K}_{0},{V}_{1},{V}_{0}\right\}\), i.e., \({\dot{W}}_{l}\propto -{\partial }_{{W}_{l}}{{{\mathcal{A}}}}\), \({\dot{K}}_{l}\propto -{\partial }_{{K}_{l}}{{{\mathcal{A}}}}\), and \({\dot{V}}_{l}\propto -{\partial }_{{V}_{l}}{{{\mathcal{A}}}}\) (for l = 1,0), provides the following synaptic plasticity rules:$$\left\{\begin{array}{c}{\dot{W}}_{1}\propto \left\langle {x}_{t}{o}_{t}^{{{{\rm{T}}}}}\right\rangle -\left\langle {x}_{t}{\vec{1}}^{{{{\rm{T}}}}}\right\rangle \odot {{{\rm{sig}}}}\left({W}_{1}\right)\\ {\dot{W}}_{0}\propto \left\langle \bar{{x}_{t}}{o}_{t}^{{{{\rm{T}}}}}\right\rangle -\left\langle \bar{{x}_{t}}{\vec{1}}^{{{{\rm{T}}}}}\right\rangle \odot {{{\rm{sig}}}}\left({W}_{0}\right)\\ {\dot{K}}_{1}\propto \left\langle {x}_{t}{{\psi }_{t-1}^{x}}^{{{{\rm{T}}}}}\right\rangle -\left\langle {x}_{t}{\vec{1}}^{{{{\rm{T}}}}}\right\rangle \odot {{{\rm{sig}}}}\left({K}_{1}\right)\\ {\dot{K}}_{0}\propto \left\langle \bar{{x}_{t}}{{\psi }_{t-1}^{x}}^{{{{\rm{T}}}}}\right\rangle -\left\langle \bar{{x}_{t}}{\vec{1}}^{{{{\rm{T}}}}}\right\rangle \odot {{{\rm{sig}}}}\left({K}_{0}\right)\\ {\dot{V}}_{1}\propto \left\langle \left(\vec{1}-2{\varGamma }_{t}\right)\odot {y}_{t}{{\psi }_{t-1}^{y}}^{{{{\rm{T}}}}}\right\rangle -\left\langle {y}_{t}{\vec{1}}^{{{{\rm{T}}}}}\right\rangle \odot {{{\rm{sig}}}}\left({V}_{1}\right)-\frac{\partial_{V_1}{\mathcal{C}}}{t}\\ {\dot{V}}_{0}\propto \left\langle \left(\vec{1}-2{\varGamma }_{t}\right)\odot \bar{{y}_{t}}{{\psi }_{t-1}^{y}}^{{{{\rm{T}}}}}\right\rangle -\left\langle \bar{{y}_{t}}{\vec{1}}^{{{{\rm{T}}}}}\right\rangle \odot {{{\rm{sig}}}}\left({V}_{0}\right)-\frac{\partial_{V_0}{\mathcal{C}}}{t}\end{array}\right.$$
                    (18)
                The three-factor Hebbian plasticity in V enables updates of (mental) actions to minimise the risks associated with future outcomes. Neuronal substrates such as neuromodulators31,32,33 may encode the risk. This modulated plasticity ensures that only good strategies are accepted, and bad strategies are rejected, enabling efficient rewriting of binary memory.Previous works established that the dynamics of canonical neural networks that minimise a cost function are cast as variational free energy minimisation under a class of factorial POMDP models19,20, by showing a natural equivalence between the Helmholtz energy \({{{\mathcal{A}}}}\) and variational free energy \({{{\mathcal{F}}}}\). In other words, there exists a generative model that satisfies \({{{\mathcal{A}}}}{{\equiv }}{{{\mathcal{F}}}}\), where \(\pi \left(\varphi \right)\) encodes or parameterises the posterior belief \(Q\left(\vartheta \right)\), i.e., \(\pi \left(\varphi \right)\equiv Q\left(\vartheta \right)\), and ξ encodes the generative model structure m = m(ξ). As shown previously20, Eq. (16) can be transformed into the form in Fig. 1b, in which the Hamiltonian formally corresponds to the negative logarithm of the generative model, \({{{{\mathcal{H}}}}}_{\xi }=-{{{\mathrm{ln}}}}\,\,{P}_{m}\left({o}_{1:t} , \vartheta \right)\). This indicates that any neural network that minimises \({{{\mathcal{A}}}}\) can be conceptualised as performing a variational Bayesian inference. Neural network states \(\left\{{x}_{1:t} , {y}_{1:t},\omega , \phi \right\}\) correspond to quantities in variational Bayesian inference, such as \({x}_{t} {\equiv } {{{{\bf{s}}}}}_{t}\), \({y}_{t} {\equiv } {{{{\mathbf{\delta }}}}}_{t}\), ω ≡ θ, and \({\phi \equiv \lambda }\) (Table 1). In particular, the memory (policy) matrix C corresponds to a synaptic matrix V to the output layer, and yt represents the memory readout at position \({\psi }_{t-1}^{y}\). Owing to this equivalence, internal states of canonical neural networks encode the posterior belief about states of external Turing machines.However, it should be noted that arbitrarily selected neural dynamics correspond to inferences of the external milieu under a suboptimal generative model with biased prior beliefs. Thus, the generative model must be optimised by updating the genes.Evolution as active Bayesian model selectionThis section elaborates on the notion that the Helmholtz free energy minimisation derives natural selection and Bayesian model selection. To characterise the optimal gene distribution in terms of phenotypes, it is necessary to commit to a specific environmental generative process and evolutionary rule. Natural selection updates the current (i.e., prior) distribution n(ξ) to the offspring (i.e., posterior) distribution \(N\left({o}_{1:T},\xi \right)\), where T denotes the maximum lifetime for an individual. The external milieu states do not directly affect the agent’s internal states and vice versa—refer to Markov blanket62,63—and the behaviour of each individual affects other individuals only through the environment. Thus, the agents receive external information only through sensory inputs, analogous to strange particles64. This involves vision, audition, somatosensation, and interoception that may have been caused by the agent’s own action. Therefore, the reproduction rate \(\rho \left({o}_{1:T}\right)\) is defined as a function of the sensory input sequence \({o}_{1:T}\).The maximisation of the offspring number \(Z={\sum}_{{o}_{1:T},\xi }\rho \left({o}_{1:T}\right)P\left({o}_{1:T}|\xi \right)n\left(\xi \right)\) can be derived from the Helmholtz energy minimisation at the species or population level. Each agent minimises \({{{\mathcal{A}}}}\left[\pi \left(\varphi \right),{o}_{1:T},\xi \right]\) with respect to the internal state distribution \(\pi \left(\varphi \right)\). This process can be extended to the population or species level. When doing so, there is a degree of freedom to add an arbitrary function of \({o}_{1:T}\) and ξ, denoted as \(\bar{{{{\mathcal{C}}}}}\left({o}_{1:T},\xi \right)\), to the Helmholtz energy as the integration constant, because the variational solution of \(\pi \left(\varphi \right)\) is unchanged by this modification. Thus, the ensemble Helmholtz energy generally has the following form:$$\bar{{{{\mathcal{A}}}}}\left[\pi \left({o}_{1:T},\xi \right)\right]={\left\langle {{{\mathcal{A}}}}\left[\pi \left(\varphi \right),{o}_{1:T},\xi \right]+\bar{{{{\mathcal{C}}}}}\left({o}_{1:T},\xi \right)+{{{\mathrm{ln}}}}\,\pi \left({o}_{1:T},\xi \right)\right\rangle }_{\pi \left({o}_{1:T},\xi \right)}$$
                    (19)
                where \({\left\langle \cdot \right\rangle }_{\pi \left({o}_{1:T},\xi \right)}={\sum}_{{o}_{1:T},\xi }\,\cdot \,\pi \left({o}_{1:T},\xi \right)\) is the expectation over \(\pi \left({o}_{1:T},\xi \right)\). The integration constant \({\bar{{{{\mathcal{C}}}}}}\left({o}_{1:T},\xi \right)\) can be read as an arbitrary evolution rule. Because the integration constant \({{{\mathcal{C}}}}\) involved in equation (3) is selected to satisfy \(\int {e}^{{{{\mathcal{-}}}}{{{\mathcal{A}}}}}d{o}_{1:t}=1\) at the steady state, \({{{\mathcal{A}}}}\left[\pi \left(\varphi \right),{o}_{1:t},\xi \right]\ge -{{{\mathrm{ln}}}}\,{P}_{m}\left({o}_{1:t}\right)\) holds. As optimising the internal states \(\pi \left(\varphi \right)\) provides \({{{\mathcal{A}}}}\left[\pi \left(\varphi \right),{o}_{1:t},\xi \right]\simeq -{{{\mathrm{ln}}}}\,{P}_{m}\left({o}_{1:t}\right)\), solving the variation \(\delta \bar{{{{\mathcal{A}}}}}=0\) with respect to \(\pi \left({o}_{1:T},\xi \right)\) results in \(\pi \left({o}_{1:T},\xi \right)\propto {P}_{m}\left({o}_{1:T}\right){e}^{{{{\mathcal{-}}}}\bar{{{{\mathcal{C}}}}}\left({o}_{1:T},\xi \right)}\).When genes encode generative models with sufficient representation capacity, \({P}_{m}\left({o}_{1:t}\right)\) provides a good approximation of \(P\left({o}_{1:t}{|m}\right)\). Moreover, \(P\left({o}_{1:t}{|m}\right)\equiv P\left({o}_{1:t}|\xi \right)\) holds by construction because model structure \(m=m\left(\xi \right)\) is a function of ξ. To rephrase, as \({{{{\mathcal{H}}}}}_{\xi }\) can take various form depending on gene ξ, there exists a subset of genes that ensure the matching between \({P}_{m}\left({o}_{1:t}\right)\) and \(P\left({o}_{1:t}|\xi \right)\). Although initial genes may not be involved in this subset, \({\left\langle -{{{\mathrm{ln}}}}\,{P}_{m}\left({o}_{1:t}\right)\right\rangle }_{P\left({o}_{1:t},\xi \right)}\ge {\left\langle -{{\mathrm{ln}}}\,P\left({o}_{1:t}|\xi \right)\right\rangle }_{P\left({o}_{1:t},\xi \right)}\) is satisfied from the nonnegativity of the Kullback–Leibler divergence. Thus, the ensemble Helmholtz energy minimisation facilitates the selection of genes that satisfy \({P}_{m}\left({o}_{1:t}\right)\equiv P\left({o}_{1:t}|\xi \right)\).Hence, when \(\bar{{{{\mathcal{C}}}}}\left({o}_{1:T},\xi \right)\equiv -{{{\mathrm{ln}}}}\,\,\rho \left({o}_{1:T}\right) - {{{\mathrm{ln}}}}\,\,n\left(\xi \right)\) is substituted, \(\pi \left({o}_{1:T},\xi \right)\) corresponds to Eq. (6). Because \(\bar{{{{\mathcal{A}}}}}\simeq -{{\mathrm{ln}}}Z\) holds with the optimal \(\pi \left({o}_{1:T},\xi \right)\), this facilitates the most efficient reproduction. The total offspring number Z is maximised by minimising \({{{\mathcal{A}}}}\) owing to Eq. (9). Therefore, natural selection can be derived from the Helmholtz energy minimisation.The variational free energy corresponding to the ensemble Helmholtz energy \(\bar{{{{\mathcal{A}}}}}\) is given as$$\bar{{{{\mathcal{F}}}}}\left[Q\left({o}_{1:T},m\right)\right]={\left\langle {{{\mathcal{F}}}}\left[Q\left(\vartheta \right),{o}_{1:T},m\right] + {\bar{{{{\mathcal{C}}}}}}\left({o}_{1:T},m \right) +{{{\mathrm{ln}}}}\,\,Q\left({o}_{1:T},m\right)\right\rangle }_{Q\left({o}_{1:T},m\right)}$$
                    (20)
                where \({\left\langle \cdot \right\rangle }_{Q\left({o}_{1:T},m \right)}\) indicates the expectation over \(Q\left({o}_{1:T},m\right)\) and \({\bar{{{{\mathcal{C}}}}}}\left({o}_{1:T},m \right)=-{{{\mathrm{ln}}}}\,\rho \left({o}_{1:T}\right)-{{{\mathrm{ln}}}}\,\,n\left(m\right)\) denotes a bias term associated with the prior belief. Solving the variation \(\delta \bar{{{{\mathcal{F}}}}}=0\) provides the approximate posterior distribution \(Q\left({o}_{1:T},m\right)\propto {P}_{m}\left({o}_{1:T}\right)\rho \left({o}_{1:T}\right)n\left(m\right)\). The optimal gene distribution that minimises \(\bar{{{{\mathcal{F}}}}}\) guarantees the matching between the generative model and true generative process, \({P}_{m}\left({o}_{1:t},\vartheta \right)\equiv P\left({o}_{1:t},\vartheta \right)\), referred to as Hamiltonian matching21. This provides the optimal decision making for survival and reproduction, which makes their reproduction rate superior to that of other genes. Therefore, natural selection can be viewed as an active Bayesian model selection. When the gene mutation probability is sufficiently small, sharp peaks appear at the optimal genes that maximise the offspring number, in which multiple peaks represent equally good solutions.CoevolutionThe aforementioned framework can be extended to the coevolution of multiple agents. For simplicity, a case in which N agents receive shared sensory inputs \({o}_{t}=\left\{{o}_{t}^{\left({env}\right)},{\delta }_{t}^{\left(1\right)},\ldots ,{\delta }_{t}^{\left(N\right)}\right\}\) is considered, where \({\delta }_{t}^{\left(i\right)}\) denotes the action or output of the i-th agent and \({o}_{t}^{\left({env}\right)}\) denotes signals generated by the environment.In each generation, each agent updates the input and gene distribution from \(P({o}_{1:T},m)\) to \(Q({o}_{1:T},m)\propto P({o}_{1:T},m){\rho }_{i}{({o}_{1:T})}^{\varepsilon }\), where ε is a small positive constant and \(P\left({o}_{1:T},m\right)\) can be read as a prior distribution. The reproduction rate \({\rho }_{i}({o}_{1:T})\) of each agent can be decomposed into cooperative \({\rho }^{* }({o}_{1:T})\) and competitive \(\Delta {\rho }_{i}({o}_{1:T})\) components, which satisfy \({\rho }_{i}={\rho }^{* }+\Delta {\rho }_{i}\) and \({\sum }_{i=1}^{N}\Delta {\rho }_{i}=0\). The competitive component corresponds to a zero-sum game. In this setup, for one round of natural selection, the posterior (offspring) distribution is expressed as$$Q\left({o}_{1:T},m\right)\propto P\left({o}_{1:T}{|m}\right)P\left(m\right){\prod }_{i=1}^{N}{\left({\rho }^{* }+\Delta {\rho }_{i}\right)}^{\varepsilon }$$
                    (21)
                The limit of this evolutionary process can be analysed as follows: from the AM-GM inequality, \({\rho }^{* }{\left({o}_{1:T}\right)}^{N}\ge {\prod }_{i=1}^{N}{\rho }_{i}\left({o}_{1:T}\right)\) holds. This indicates that only the agent with the cooperative term alone (i.e., ξ with \(\Delta {\rho }_{i}=0\)) becomes the primary trend, whereas the competitive term hinders maximising the reproduction. Thus, when the gene mutation probability is sufficiently small, Eq. (21) asymptotically converges to a steady-state distribution with sharp peaks, \(Q\left({o}_{1:T},m\right)\propto {{{\mathrm{lim}}}}_{n\to \infty }{{\rho }^{* }\left({o}_{1:T}\right)}^{n}\), which is characterised only by the cooperative component. These observations imply that survival agents select strategies that are beneficial for the survival of the species, but not for the individual.Numerical simulationsIn Fig. 2, canonical neural networks inferred states of external adders. Based on handwritten digit inputs, the networks inferred the hidden states (st) and memory (C) and learned state transition mapping (B). For simplicity, the likelihood mapping (A) was given a priori. The update rules were given as the gradient descent on \({{{\mathcal{A}}}}\), as shown in Eq. (15), which include the state update of the automaton and memory readout. Here, \({{\psi }^{x}_{t}}={{x}_{t}}\) was used. Reading header movement was expressed by basis functions \({\psi }_{t}^{y }\). For simplicity, random-access memory that can freely move header position, expressed as \({\psi }_{t}^{y }={H}^{y }{x}_{t}\), was adopted in this simulation. Memory writing occurred through the risk modulated Hebbian plasticity of V following Eq. (14), where risk \({\varGamma }_{t}={{{\rm{sig}}}}({({\psi }_{t1}^{\gamma },{\psi }_{t2}^{\gamma }+{\psi }_{t3}^{\gamma }+{\psi }_{t4}^{\gamma })}^{{{{\rm{T}}}}})\) was characterised by bases \({\psi }_{t}^{\gamma }={({s}_{t,1:16}^{{{{\rm{T}}}}}{s}_{t,17:32},{s}_{t-{{\mathrm{1,1}}}:16}^{{{{\rm{T}}}}}{s}_{t-{{\mathrm{1,17}}}:32},{\delta }_{t-1},{\varGamma }_{t-1})}^{{{{\rm{T}}}}}\).In Fig. 3, the reproduction rate varied depending on the performance of estimating the summation of input binary numbers. Agents employed a genetically encoded risk Γt defined as a function of bases, \({\varGamma }_{t}={{{\rm{sig}}}}\left({\left({\xi }_{1}{\psi }_{t1}^{\gamma }+{\xi }_{3}{\psi }_{t2}^{\gamma }+{\xi }_{5}{\psi }_{t3}^{\gamma }+{\xi }_{7}{\psi }_{t4}^{\gamma },{\xi }_{2}{\psi }_{t1}^{\gamma }+{\xi }_{4}{\psi }_{t2}^{\gamma }+{\xi }_{6}{\psi }_{t3}^{\gamma }+{\xi }_{8}{\psi }_{t4}^{\gamma },\right)}^{{{{\rm{T}}}}}\right)\), which was characterised by an eight-bit gene vector ξ ∈ {0,1}8. When the corresponding gene adopted the value of 1, the term was included in the risk. When a gene bears ξ = (1,0,0,1,0,1,0,1)T, the risk suits to act as an adder and thus maximise the reproduction rate. The gene distribution was initialised with a uniform distribution. Natural selection was depicted as an operation that adds small mutation to the parent gene distribution n(ξ) to create n′(ξ) and then replaces n′(ξ) with the offspring gene distribution \(\pi \left({o}_{1:t},\xi \right)\) following Eq. (6). This process was iterated for 40 generations.In Fig. 4, the environment was determined by 10 machines, each of which involved 10-dimensional hidden states (st) and two-dimensional mental action (δt) in the form of a POMDP, in which st and δt are one-hot vectors. The transition matrix B ∈ {0,1}10 × 10 × 2 was randomly generated for each machine, whereas the likelihood mapping A ∈ [0,1]10 × 10 was randomly generated and commonly used for all machines. The canonical neural network employed two actions yt and \({y}_{t}^{{\prime} }\), where yt encoded memory readout from V, while \({y}_{t}^{{\prime} }\) encoded readout from V′ that memorises transition mappings of external machines.Reporting summaryFurther information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

## Data availability

All relevant data are presented in this paper. Figures 2–4 were generated using the author’s scripts (see Code Availability).


## Code availability

The MATLAB scripts are available at https://github.com/takuyaisomura/reverse_engineering. The scripts are covered under the GNU General Public Licence v3.0.


## References
Turing, A. M. On computable numbers, with an application to the Entscheidungsproblem. Proc. Lond. Math. Soc. S2–42, 230–265 (1936).MathSciNet 
    
                    Google Scholar 
                Hodgkin, A. L. & Huxley, A. F. A quantitative description of membrane current and its application to conduction and excitation in nerve. J. Physiol. 117, 500–544 (1952).Article 
    
                    Google Scholar 
                FitzHugh, R. Impulses and physiological states in theoretical models of nerve membrane. Biophys. J. 1, 445–466 (1961).Article 
    ADS 
    
                    Google Scholar 
                Nagumo, J., Arimoto, S. & Yoshizawa, S. An active pulse transmission line simulating nerve axon. Proc. IRE 50, 2061–2070 (1962).Article 
    
                    Google Scholar 
                Hebb, D. O. The Organization of Behavior: A Neuropsychological Theory (Wiley, New York, 1949).Song, S., Miller, K. D. & Abbott, L. F. Competitive Hebbian learning through spike-timing-dependent synaptic plasticity. Nat. Neurosci. 3, 919–926 (2000).Article 
    
                    Google Scholar 
                Clopath, C., Büsing, L., Vasilaki, E. & Gerstner, W. Connectivity reflects coding: a model of voltage-based STDP with homeostasis. Nat. Neurosci. 13, 344–352 (2010).Article 
    
                    Google Scholar 
                Roth, G. & Dicke, U. Evolution of the brain and intelligence. Trends Cogn. Sci. 9, 250–257 (2005).Article 
    
                    Google Scholar 
                Rabinovich, M. I., Varona, P., Selverston, A. I. & Abarbanel, H. D. Dynamical principles in neuroscience. Rev. Mod. Phys. 78, 1213–1265 (2006).Article 
    ADS 
    
                    Google Scholar 
                Sussillo, D. & Abbott, L. F. Generating coherent patterns of activity from chaotic neural networks. Neuron 63, 544–557 (2009).Article 
    
                    Google Scholar 
                Laje, R. & Buonomano, D. V. Robust timing and motor patterns by taming chaos in recurrent neural networks. Nat. Neurosci. 16, 925–933 (2013).Article 
    
                    Google Scholar 
                Knill, D. C. & Pouget, A. The Bayesian brain: the role of uncertainty in neural coding and computation. Trends Neurosci. 27, 712–719 (2004).Article 
    
                    Google Scholar 
                Doya, K., Ishii, S., Pouget, A. & Rao, R. P. (Eds.) Bayesian Brain: Probabilistic Approaches to Neural Coding (MIT Press, Cambridge, MA, USA, 2007).Linsker, R. Self-organization in a perceptual network. Computer 21, 105–117 (1988).Article 
    
                    Google Scholar 
                Dayan, P., Hinton, G. E., Neal, R. M. & Zemel, R. S. The Helmholtz machine. Neural Comput 7, 889–904 (1995).Article 
    
                    Google Scholar 
                Sutton, R. S. & Barto, A. G. Reinforcement Learning (Cambridge, MA: MIT Press, 1998).Friston, K. J., Kilner, J. & Harrison, L. A free energy principle for the brain. J. Physiol. Paris 100, 70–87 (2006).Article 
    
                    Google Scholar 
                Friston, K. J. The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127–138 (2010).Article 
    
                    Google Scholar 
                Isomura, T. & Friston, K. J. Reverse-engineering neural networks to characterize their cost functions. Neural Comput. 32, 2085–2121 (2020).Article 
    MathSciNet 
    
                    Google Scholar 
                Isomura, T., Shimazaki, H. & Friston, K. J. Canonical neural networks perform active inference. Commun. Biol. 5, 55 (2022).Article 
    
                    Google Scholar 
                Isomura, T. Bayesian mechanics of self-organising systems. Preprint at arXiv arXiv:2311.10216. https://arxiv.org/abs/2311.10216 (2023).Tazawa, U. T. & Isomura, T. Synaptic pruning facilitates online Bayesian model selection. Preprint at bioRxiv https://www.biorxiv.org/content/10.1101/2024.05.15.593712v1 (2024).Isomura, T., Kotani, K. & Jimbo, Y. Cultured cortical neurons can perform blind source separation according to the free-energy principle. PLoS Comput. Biol. 11, e1004643 (2015).Article 
    ADS 
    
                    Google Scholar 
                Isomura, T. & Friston, K. J. In vitro neural networks minimise variational free energy. Sci. Rep. 8, 16926 (2018).Article 
    ADS 
    
                    Google Scholar 
                Isomura, T., Kotani, K., Jimbo, Y. & Friston, K. J. Experimental validation of the free-energy principle with in vitro neural networks. Nat. Commun. 14, 4547 (2023).Article 
    ADS 
    
                    Google Scholar 
                Graves, A. Neural turing machines. Preprint at arXiv arXiv:1410.5401. https://arxiv.org/abs/1410.5401 (2014).Graves, A. et al. Hybrid computing using a neural network with dynamic external memory. Nature 538, 471–476 (2016).Article 
    ADS 
    
                    Google Scholar 
                Cabessa, J. Turing complete neural computation based on synaptic plasticity. PLoS ONE 14, e0223451 (2019).Article 
    
                    Google Scholar 
                Cabessa, J. & Tchaptchet, A. Automata complete computation with Hodgkin–Huxley neural networks composed of synfire rings. Neural Netw. 126, 312–334 (2020).Article 
    
                    Google Scholar 
                Plank, J., Zheng, C., Schuman, C. & Dean, C. Spiking neuromorphic networks for binary tasks. Int. Conf. Neuromorphic Syst. 22, 1–9 (2021).
                    Google Scholar 
                Pawlak, V., Wickens, J. R., Kirkwood, A. & Kerr, J. N. Timing is not everything: neuromodulation opens the STDP gate. Front. Syn. Neurosci. 2, 146 (2010).Article 
    
                    Google Scholar 
                Frémaux, N. & Gerstner, W. Neuromodulated spike-timing-dependent plasticity, and theory of three-factor learning rules. Front. Neural Circuits 9, 85 (2016).Article 
    
                    Google Scholar 
                Kuśmierz, Ł., Isomura, T. & Toyoizumi, T. Learning with three factors: modulating Hebbian plasticity with errors. Curr. Opin. Neurobiol. 46, 170–177 (2017).Article 
    
                    Google Scholar 
                Wald, A. An essentially complete class of admissible decision functions. Ann. Math. Stat. 18, 549–555 (1947).Article 
    MathSciNet 
    
                    Google Scholar 
                Brown, L. D. A complete class theorem for statistical problems with finite-sample spaces. Ann. Stat. 9, 1289–1300 (1981).Article 
    ADS 
    MathSciNet 
    
                    Google Scholar 
                Berger, J. O. Statistical Decision Theory and Bayesian Analysis (Springer Science & Business Media, Berlin, 2013).Blei, D. M., Kucukelbir, A. & McAuliffe, J. D. Variational inference: a review for statisticians. J. Am. Stat. Assoc. 112, 859–877 (2017).Article 
    MathSciNet 
    
                    Google Scholar 
                Friston, K. J., FitzGerald, T., Rigoli, F., Schwartenbeck, P. & Pezzulo, G. Active inference: a process theory. Neural Comput 29, 1–49 (2017).Article 
    MathSciNet 
    
                    Google Scholar 
                Sajid, N., Ball, P. J., Parr, T. & Friston, K. J. Active inference: demystified and compared. Neural Comput. 33, 674–712 (2021).Article 
    
                    Google Scholar 
                Parr, T., Pezzulo, G. & Friston, K. J. Active Inference: The Free Energy Principle in Mind, Brain, and Behavior. (MIT Press, Cambridge, MA, USA, 2022).Isomura, T., Parr, T. & Friston, K. J. Bayesian filtering with multiple internal models: toward a theory of social intelligence. Neural Comput. 31, 2390–2431 (2019).Article 
    
                    Google Scholar 
                Kobayashi, T. J. & Sughiyama, Y. Fluctuation relations of fitness and information in population dynamics. Phys. Rev. Lett. 115, 238102 (2015).Article 
    ADS 
    
                    Google Scholar 
                Kobayashi, T. J. & Sughiyama, Y. Stochastic and information-thermodynamic structures of population dynamics in a fluctuating environment. Phys. Rev. E 96, 012402 (2017).Article 
    ADS 
    
                    Google Scholar 
                Nakashima, S. & Kobayashi, T. J. Acceleration of evolutionary processes by learning and extended Fisher’s fundamental theorem. Phys. Rev. Res. 4, 013069 (2022).Article 
    
                    Google Scholar 
                Shimazaki, H. & Niebur, E. Phase transitions in multiplicative competitive processes. Phys. Rev. E 72, 011912 (2005).Article 
    ADS 
    
                    Google Scholar 
                Kirchhoff, M., Parr, T., Palacios, E., Friston, K. J. & Kiverstein, J. The Markov blankets of life: autonomy, active inference and the free energy principle. J. R. Soc. Interface 15, 20170792 (2018).Article 
    
                    Google Scholar 
                Constant, A., Ramstead, M. J. D., Veissiere, S. P., Campbell, J. O. & Friston, K. J. A variational approach to niche construction. J. R. Soc. Interface 15, 20170685 (2018).Article 
    
                    Google Scholar 
                Ramstead, M. J. D., Constant, A., Badcock, P. B. & Friston, K. J. Variational ecology and the physics of sentient systems. Phys. Life Rev. 31, 188–205 (2019).Article 
    ADS 
    
                    Google Scholar 
                Pezzulo, G., Parr, T. & Friston, K. J. The evolution of brain architectures for predictive coding and active inference. Philos. Trans. R. Soc. B 377, 20200531 (2022).Article 
    
                    Google Scholar 
                Rogozhin, Y. Small universal turing machines. Theor. Comput. Sci. 168, 215–240 (1996).Article 
    MathSciNet 
    
                    Google Scholar 
                Neary, T. & Woods, D. Four small universal Turing machines. Fundam. Inform. 91, 123–144 (2009).Mirza, M. B., Adams, R. A., Friston, K. J. & Parr, T. Introducing a Bayesian model of selective attention based on active inference. Sci. Rep. 9, 13915 (2019).Article 
    ADS 
    
                    Google Scholar 
                Gershman, S. J. et al. Explaining dopamine through prediction errors and beyond. Nat. Neurosci. 27, 1645–1655 (2024).Article 
    
                    Google Scholar 
                Neves, G., Cooke, S. F. & Bliss, T. V. Synaptic plasticity, memory and the hippocampus: a neural network approach to causality. Nat. Rev. Neurosci. 9, 65–75 (2008).Article 
    
                    Google Scholar 
                Bittner, K. C., Milstein, A. D., Grienberger, C., Romani, S. & Magee, J. C. Behavioral time scale synaptic plasticity underlies CA1 place fields. Science 357, 1033–1036 (2017).Article 
    ADS 
    
                    Google Scholar 
                Wu, Y. & Maass, W. A simple model for behavioral time scale synaptic plasticity (BTSP) provides content addressable memory with binary synapses and one-shot learning. Nat. Commun. 16, 342 (2025).Article 
    
                    Google Scholar 
                Isomura, T. Quadratic speedup of global search using a biased crossover of two good solutions. Preprint at arXiv arXiv:2111.07680. https://arxiv.org/abs/2111.07680 (2021).Friston, K. & Frith, C. A duet for one. Conscious. Cogn. 36, 390–405 (2015).Article 
    
                    Google Scholar 
                Friston, K. J. et al. Federated inference and belief sharing. Neurosci. Biobehav. Rev. 156, 105500 (2023).Article 
    
                    Google Scholar 
                Friston, K. J., Stephan, K. E., Montague, R. & Dolan, R. J. Computational psychiatry: the brain as a phantastic organ. Lancet Psychiatry 1, 148–158 (2014).Article 
    
                    Google Scholar 
                Newsome, W. T., Britten, K. H. & Movshon, J. A. Neuronal correlates of a perceptual decision. Nature 341, 52–54 (1989).Article 
    ADS 
    
                    Google Scholar 
                Da Costa, L. et al. Active inference on discrete state-spaces: a synthesis. J. Math. Psychol. 99, 102447 (2020).Article 
    MathSciNet 
    
                    Google Scholar 
                Friston, K. J. et al. The free energy principle made simpler but not too simple. Phys. Rep. 1024, 1–29 (2023).Article 
    ADS 
    MathSciNet 
    
                    Google Scholar 
                Friston, K. J. et al. Path integrals, particular kinds, and strange things. Phys. Life Rev. 47, 35–62 (2023).Article 
    ADS 
    
                    Google Scholar 
                Download references

## Acknowledgements
T.I. is supported by the Japan Society for the Promotion of Science (JSPS) KAKENHI Grant Number JP23H04973 and the Japan Science and Technology Agency (JST) CREST Grant Number JPMJCR22P1. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.

## Author information
Authors and AffiliationsBrain Intelligence Theory Unit, RIKEN Center for Brain Science, Saitama, JapanTakuya IsomuraAuthorsTakuya IsomuraView author publicationsYou can also search for this author inPubMed Google ScholarContributionsThis work was done by T. Isomura.Corresponding authorCorrespondence to
                Takuya Isomura.